{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "### Reference\n",
        "https://github.com/huggingface/diffusers/blob/main/examples/dreambooth/README_sdxl.md"
      ],
      "metadata": {
        "id": "RenqEjOCUvdA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Check CUDA"
      ],
      "metadata": {
        "id": "1edBn8ByZkOB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import subprocess\n",
        "# GPU Check\n",
        "\n",
        "# @markdown The system checks for a compatible GPU with enough memory and installs necessary Python packages during setup.\n",
        "# Run the nvidia-smi command to get the VRAM information\n",
        "result = subprocess.run([\"nvidia-smi\", \"--query-gpu=name,memory.total,memory.free\",\n",
        "                        \"--format=csv,noheader\"], capture_output=True, check=True)\n",
        "\n",
        "# Split the output by newline characters to get a list of VRAM info for each GPU\n",
        "vram_info = result.stdout.decode(\"utf-8\").strip().split(\"\\n\")\n",
        "\n",
        "# Parse the VRAM info for each GPU\n",
        "for info in vram_info:\n",
        "    name, total, free = info.split(\",\")\n",
        "    total = int(total.strip().split()[0])  # Total VRAM in MB\n",
        "    free = int(free.strip().split()[0])  # Free VRAM in MB\n",
        "\n",
        "    print(f\"GPU: {name}, Total VRAM: {total} MB, Free VRAM: {free} MB\")\n",
        "\n",
        "if total < 15109:  # 15109MB is equivalent to 15GB\n",
        "    # Display an error message in red text\n",
        "    print(\"\\033[91mError: Not enough VRAM available. Please change the runtime to a GPU with at least 15GB VRAM.\\033[0m\")\n",
        "    raise SystemExit\n",
        "else:\n",
        "    print(\"\\033[92mYou have enough VRAM to continue\\033[0m\")\n"
      ],
      "metadata": {
        "id": "iyjWsdkwaWtA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Get CUDA version"
      ],
      "metadata": {
        "id": "yhShWI0BeuJn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!nvcc -V"
      ],
      "metadata": {
        "id": "_LWlTggyef3_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%pwd"
      ],
      "metadata": {
        "id": "bMdKZ02BPCNi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Install dependencies"
      ],
      "metadata": {
        "id": "tnUK4IoKMCea"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Install autotrain\n",
        "%pip install -U autotrain-advanced"
      ],
      "metadata": {
        "id": "K1U_eCDEME1i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# setup will install some dependencies\n",
        "!autotrain setup"
      ],
      "metadata": {
        "id": "KgmvDZAzMReS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Auotrain help"
      ],
      "metadata": {
        "id": "sG40kLNPqZkP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!autotrain dreambooth --help"
      ],
      "metadata": {
        "id": "ajArbhd8qYAy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Toy example\n",
        "\n",
        "Now let's get our dataset. For this example we will use some dog images: https://huggingface.co/datasets/diffusers/dog-example.\n",
        "\n",
        "Let's first download it locally:"
      ],
      "metadata": {
        "id": "t3gof50BOCjJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%pwd"
      ],
      "metadata": {
        "id": "_RYlI1OJQ9Sw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from huggingface_hub import snapshot_download\n",
        "\n",
        "local_dir = \"./dog\"\n",
        "snapshot_download(\n",
        "    \"diffusers/dog-example\",\n",
        "    local_dir=local_dir, repo_type=\"dataset\",\n",
        "    ignore_patterns=\".gitattributes\",\n",
        ")"
      ],
      "metadata": {
        "id": "g8dG-2XQOGnK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%pwd"
      ],
      "metadata": {
        "id": "WWJoqPAbsAfz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%ls"
      ],
      "metadata": {
        "id": "EpDp7Vw1sEFG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Set environment variables"
      ],
      "metadata": {
        "id": "bS1Wswy04fdo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "def set_env_variables(project_name = 'test', image_path = 'training_images'):\n",
        "\n",
        "    # Stable diffusion model\n",
        "    os.environ[\"MODEL_NAME\"] = \"stabilityai/stable-diffusion-xl-base-1.0\"\n",
        "    #os.environ[\"INSTANCE_DIR\"] = \"dog\"\n",
        "    # The model name\n",
        "    os.environ[\"PROJECT_NAME\"] = project_name\n",
        "    #os.environ[\"VAE_PATH\"] = \"madebyollin/sdxl-vae-fp16-fix\"\n",
        "    # Training image's dir\n",
        "    os.environ[\"IMAGE_PATH\"]\n",
        "\n",
        "# Test the function by calling it\n",
        "set_env_variables('test-0')\n",
        "\n",
        "# You can now access these environment variables like this:\n",
        "model_name = os.environ[\"MODEL_NAME\"]\n",
        "project_name = os.environ[\"PROJECT_NAME\"]\n",
        "image_path = os.environ[\"IMAGE_PATH\"]\n",
        "\n",
        "print(f\"MODEL_NAME: {model_name}\")\n",
        "print(f\"PROJECT_NAME: {project_name}\")\n",
        "print(f\"IMAGE_PATH: {image_path}\")\n"
      ],
      "metadata": {
        "id": "dA4EJnhW4lkn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Run dreambooth training"
      ],
      "metadata": {
        "id": "EqXL9YUoOmPZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!autotrain dreambooth \\\n",
        "  --model $MODEL_NAME \\\n",
        "  --image-path $IMAGE_PATH \\\n",
        "  --project-name $PROJECT_NAME \\\n",
        "  --prompt \"photo of sks man\" \\\n",
        "  --resolution 1024 \\\n",
        "  --batch-size 1 \\\n",
        "  --num-steps 500 \\\n",
        "  --fp16 \\\n",
        "  --gradient-accumulation 4 \\\n",
        "  --lr 1e-4\n",
        "# --push-to-hub\n",
        "# --hub-model-id username/sdxl-dreambooth-model\n"
      ],
      "metadata": {
        "id": "7ww0SM14nQeq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "! sudo rm -r output_refined"
      ],
      "metadata": {
        "id": "f6RL6ZzMEtBi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Inference"
      ],
      "metadata": {
        "id": "Hex40A5mxZs0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "# dir for generated images\n",
        "OUTPUT_DIR = 'output_images'\n",
        "# dir for refined images\n",
        "OUTPUT_DIR_REFINED = OUTPUT_DIR+'_refined'\n",
        "os.makedirs(OUTPUT_DIR,exist_ok = True)\n",
        "os.makedirs(OUTPUT_DIR_REFINED,exist_ok = True)"
      ],
      "metadata": {
        "id": "D5pN7mawxwzw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from diffusers import DiffusionPipeline, StableDiffusionXLImg2ImgPipeline\n",
        "import torch\n",
        "\n",
        "model = \"stabilityai/stable-diffusion-xl-base-1.0\"\n",
        "pipe = DiffusionPipeline.from_pretrained(\n",
        "    model,\n",
        "    torch_dtype=torch.float16,\n",
        ")\n",
        "pipe.to(\"cuda\")\n",
        "project_name  = os.getenv('PROJECT_NAME')\n",
        "model_name = os.getenv('MODEL_NAME')\n",
        "pipe.load_lora_weights(project_name,\n",
        "                       weight_name=\"pytorch_lora_weights.safetensors\")\n",
        "\n",
        "refiner = StableDiffusionXLImg2ImgPipeline.from_pretrained(\n",
        "    model_name,\n",
        "    torch_dtype=torch.float16,\n",
        ")\n",
        "refiner.to(\"cuda\")\n",
        "\n",
        "\n",
        "prompt = \"a photo of sks dog, pixar, cartoon, 3d, headshots, fantasy, 4k, uhd\"\n",
        "\n",
        "for seed in range(10):\n",
        "    generator = torch.Generator(\"cuda\").manual_seed(seed)\n",
        "    image = pipe(prompt=prompt,\n",
        "                 generator=generator,\n",
        "                 num_inference_steps=25)\n",
        "    image = image.images[0]\n",
        "    image.save(f\"{OUTPUT_DIR}/{seed}.png\")\n",
        "    image = refiner(prompt=prompt, generator=generator, image=image)\n",
        "    image = image.images[0]\n",
        "    image.save(f\"{OUTPUT_DIR_REFINED}/{seed}.png\")\n",
        "\n"
      ],
      "metadata": {
        "id": "rYNFVkcKnQpn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Zip output dirs\n",
        "Easily download the generated images.\n"
      ],
      "metadata": {
        "id": "JC6GDVPDG-Ql"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import zipfile\n",
        "\n",
        "import zipfile\n",
        "import os\n",
        "\n",
        "def zip_folder(folder_name):\n",
        "    try:\n",
        "        # Generate the output zip filename by appending '.zip' to the folder name\n",
        "        output_zip_filename = f\"{folder_name}.zip\"\n",
        "\n",
        "        with zipfile.ZipFile(output_zip_filename, 'w', zipfile.ZIP_DEFLATED) as zipf:\n",
        "            for root, dirs, files in os.walk(folder_name):\n",
        "                for file in files:\n",
        "                    file_path = os.path.join(root, file)\n",
        "                    arcname = os.path.relpath(file_path, folder_name)\n",
        "                    zipf.write(file_path, arcname)\n",
        "        print(f'Folder \"{folder_name}\" has been compressed to \"{output_zip_filename}\" successfully.')\n",
        "    except Exception as e:\n",
        "        print(f'Error: {e}')\n"
      ],
      "metadata": {
        "id": "Fr_Ra7EmnQsZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Compress dirs to download\n",
        "folder_name = 'output_images'\n",
        "zip_folder(OUTPUT_DIR)"
      ],
      "metadata": {
        "id": "6gC9igHlnQvF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "zip_folder(OUTPUT_DIR_REFINED)"
      ],
      "metadata": {
        "id": "rZFKLgnenQyp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "6fwXqTJDOv16"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}